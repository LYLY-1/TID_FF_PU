# -*- coding: utf-8 -*-
"""TID_FFPU_KPI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KQ0SFjTxI7ooK4_HiO6roFcpXc7ciEhz

# Note

- 29/06/2023: Process TID
"""

import numpy as np
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
from google.colab import auth
auth.authenticate_user()
from gspread_dataframe import get_as_dataframe, set_with_dataframe
import gspread
from google.auth import default
import glob
creds, _ = default()
import os
from datetime import date, timedelta
gc = gspread.authorize(creds)
from datetime import datetime
from dateutil.relativedelta import relativedelta

def driver_finder(x):
    if 'NEXT' in x.upper(): return 'fulltime'
    elif('AGAR' in x.upper()) | (
          '518-FRLA' in x.upper()) | (
            'XDOCT' in x.upper()) | (
              'XDOCK' in x.upper()) | (
                '936-TSS' in x.upper()) | (
                  'GRAB' in x.upper()) : return '3PLs'
    elif 'FRLA' in x.upper(): return 'freelancer'
    else: return 'others'

def dispute_aloninja(data, link, start_date, end_date, sheet_name):
  df = gc.open_by_url(link).worksheet(sheet_name)
  df = get_as_dataframe(df, evaluate_formulas=True, header=0)
  lst_driver_id = df["ID"].unique()
  data.loc[(data['driver_id'].isin(lst_driver_id)) & (data['attempt_date'] >=start_date)& (data['attempt_date'] <= end_date), 'bug_app'] = 1
  data.loc[data['attempt_date']=='2023-06-17', 'bug_app']=1
  data['attempt_datetime'] = pd.to_datetime(data['attempt_datetime'])
  data.loc[data['attempt_datetime'] > '2023-06-27 17:00:00', 'bug_app']=1
  data['result_aloninja'] = data['fully_driver_result']
  data.loc[data['bug_app']==1, 'result_aloninja']='need_to_check'
  return data

def disputed_rsvn(data, link, sheet_name):
  df = gc.open_by_url(link).worksheet(sheet_name)
  df = get_as_dataframe(df, evaluate_formulas=True, header=0)
  df = df[df['Status']=='Corrected']
  df['attempt_datetime'] = pd.to_datetime(df['attempt_datetime'])
  # df = df[df['attempt_datetime'] >= '2023-06-01']
  data['final_result'] = data['result_aloninja']
  lst_rsvn = df['RSVN_id'].unique()
  data['corrected_dispute_rsvn'] = 0
  data.loc[(data['rsvn_id'].isin(lst_rsvn)), 'corrected_dispute_rsvn'] = 1
  data.loc[data['corrected_dispute_rsvn']==1, 'final_result'] = 'qualified'
  return data

def read_last_7_days(df):
  shop_b2c = gc.open_by_url('https://docs.google.com/spreadsheets/d/165yaPv4ujgG-jr9klujlqtEn1Xi1Yk3M9eDq6IuosmM/edit#gid=0').worksheet('Sheet1')
  shop_b2c = get_as_dataframe(shop_b2c, evaluate_formulas=True, header=0)
  shop_b2c = shop_b2c[['shipper_id']]
  shop_b2c = shop_b2c[~shop_b2c['shipper_id'].isna()]
  month_y = datetime.today().strftime("%m-%Y")
  lst_b2c = shop_b2c['shipper_id'].unique()
  my_path = '/content/drive/MyDrive/VN-QA/12.Fake Fail/FF_Pickup/FakeFail_PP/08.TID_data_weekly/'
  df = df[~df['tracking_id'].isna()]
  df = df.replace('-', 0)
  df.fillna(0, inplace=True)

  df = df[~df['shipper_id'].isin(lst_b2c)]
  df['order_id'] = df['order_id'].astype(str)
  lst = ['pod_photo', 'waypoint_photo_id', 'link_to_search_callee', 'reason', 'fail_pod_reason', 'fully_driver_result']
  data = pd.DataFrame({ 'attempt_date':[],'attempt_datetime':[], 'region':[], 'hub_name':[], 'hub_id':[], 'driver_name':[], 'driver_id':[], 'route_id':[], 'driver_contact':[],
                      'sales_channel':[], 'shipper_name':[], 'shipper_id':[], 'callee':[], 'rsvn_id':[], 'order_id':[], 'tracking_id':[]})
  lst_gr = ['attempt_date', 'attempt_datetime', 'region', 'hub_name', 'hub_id', 'driver_name', 'driver_id', 'route_id', 'driver_contact', 'sales_channel',
              'shipper_name', 'shipper_id', 'callee', 'rsvn_id','order_id', 'tracking_id']
  for i in lst:
    df[i] = df[i].astype(str)
    d = df.groupby(lst_gr)[i].apply(lambda x: ', '.join(x.unique())).reset_index()
    data = data.merge(d, on=lst_gr, how='right')
  lst_int = ['Fail attempt sau 10PM',
            'Không có cuộc gọi tiêu chuẩn',
            'Lịch sử tối thiểu 3 cuộc gọi ra',
            'Tối thiểu 3 cuộc gọi với thời gian đổ chuông >10s trong trường hợp khách không nghe máy',
            'Thời gian gọi sớm hơn hoặc bằng thời gian xử lý thất bại',
            'Không có cuộc gọi thành công',
            'Thời gian giữa mỗi cuộc gọi tối thiểu 1p',
            'Không có hình ảnh POD']

  for i in lst_int:
    df[i] = df[i].astype(int)
  check_tool = df.groupby(lst_gr)['Fail attempt sau 10PM',
                                  'Không có cuộc gọi tiêu chuẩn',
                                  'Lịch sử tối thiểu 3 cuộc gọi ra',
                                  'Tối thiểu 3 cuộc gọi với thời gian đổ chuông >10s trong trường hợp khách không nghe máy',
                                  'Thời gian gọi sớm hơn hoặc bằng thời gian xử lý thất bại',
                                  'Không có cuộc gọi thành công',
                                  'Thời gian giữa mỗi cuộc gọi tối thiểu 1p',
                                  'Không có hình ảnh POD'].sum().reset_index()

  count_call_log = df.groupby(lst_gr)['count_call_log'].max()
  count_valid_call_log = df.groupby(lst_gr)['count_valid_call_log'].max()
  data = data.merge(check_tool, on = lst_gr, how='inner')

  data = data.merge(count_call_log, on = lst_gr, how='inner')
  data = data.merge(count_valid_call_log, on = lst_gr, how='inner')
  # df['total_pickup_success'] = df['total_pickup_success'].astype(str)
  # total_pu_ss = df.groupby(['callee', 'driver_id'])['total_pickup_success'].apply(lambda x: ', '.join(x.unique())).reset_index()
  # data = data.merge(total_pu_ss, how = 'left', on = ['callee', 'driver_id'])
  data['driver_type'] = data.driver_name.apply(driver_finder)
  path = data['attempt_date'].unique()[0]
  data['fully_driver_result'] = data['fully_driver_result'].replace('need_to_check, fake_fail', 'need_to_check')
  data['fully_driver_result'] = data['fully_driver_result'].replace('fake_fail, need_to_check', 'need_to_check')
  # df['fully_driver_result'].unique()
  data['bug_app'] = 0
  data = dispute_aloninja(data, 'https://docs.google.com/spreadsheets/d/1mxt0ngv-m9D9tqx1uzl08d0J46mLbDExDpAy7adRV1w/edit#gid=681689770', '2023-05-21', '2023-06-22', 'QA_EXCLUDE_UPDATEON_24-05-2022')
  data = dispute_aloninja(data, 'https://docs.google.com/spreadsheets/d/1mxt0ngv-m9D9tqx1uzl08d0J46mLbDExDpAy7adRV1w/edit#gid=681689770',  '2023-06-13', '2023-06-28', 'QA_EXCLUDE_UPDATEON_12-06-2023')
  data = dispute_aloninja(data, 'https://docs.google.com/spreadsheets/d/1mxt0ngv-m9D9tqx1uzl08d0J46mLbDExDpAy7adRV1w/edit#gid=681689770',  '2023-06-22', '2023-06-28', 'QA_EXCLUDED_UPDATEON_22-06-2023')
  data = disputed_rsvn(data, 'https://docs.google.com/spreadsheets/d/1i1Rha9Qg1qZ9sGI0-ddX9QBlO6Jg9URy2tm62Fu3X20/edit?resourcekey#gid=1021540264', 'Detail_PickUp')
  # data.loc[data['attempt_date']=='2023-06-17', 'bug_app']=''
  print("--------------- DONE -----------------", path)
  data.to_csv(my_path + path +'.csv', index=False)

def pre_process(a, lst_gr):
  total_fail_tracking_id = a.groupby(lst_gr)['tracking_id'].nunique().reset_index().rename(columns={'tracking_id':'total_fail_tracking_id'})
  total_ff_tracking_id = a[a['result_aloninja']=='fake_fail'].groupby(lst_gr)['tracking_id'].nunique().reset_index().rename(columns={'tracking_id':'total_ff_tracking_id_app'})
  final_tracking_id_ff = a[a['final_result']=='fake_fail'].groupby(lst_gr)['tracking_id'].nunique().reset_index().rename(columns={'tracking_id':'final_tracking_id_ff'})
  # lst_link_pod_photo = a[a['final_result']=='fake_fail'].groupby(lst_gr)['pod_photo'].apply(', '.join).reset_index()
  lst_int = a.groupby(lst_gr)['Fail attempt sau 10PM', 'Không có cuộc gọi tiêu chuẩn',
        'Lịch sử tối thiểu 3 cuộc gọi ra',
        'Tối thiểu 3 cuộc gọi với thời gian đổ chuông >10s trong trường hợp khách không nghe máy',
        'Thời gian gọi sớm hơn hoặc bằng thời gian xử lý thất bại',
        'Không có cuộc gọi thành công',
        'Thời gian giữa mỗi cuộc gọi tối thiểu 1p', 'Không có hình ảnh POD',
        'count_call_log', 'count_valid_call_log', 'corrected_dispute_rsvn'].sum().reset_index()

  for i in [total_fail_tracking_id, total_ff_tracking_id, final_tracking_id_ff]:
    lst_int = lst_int.merge(i, how='outer', on=lst_gr)

  lst_int['tracking_id_corrected_dispue'] = lst_int['total_ff_tracking_id_app'] - lst_int['final_tracking_id_ff']
  return lst_int

def save_final_data(option, start_date, end_date):
  month_year = start_date[:7]
  lst_gr_hub = ['attempt_date', 'region', 'hub_id', 'hub_name']
  lst_gr_driver = [ 'attempt_date', 'region', 'hub_id', 'hub_name', 'driver_id', 'driver_name']
  # Đường dẫn đến thư mục chứa các file
  folder_path = '/content/drive/MyDrive/VN-QA/12.Fake Fail/FF_Pickup/FakeFail_PP/01.tracking_id_data_weekly'

  # Sử dụng glob để lấy tất cả các file trong thư mục
  files = glob.glob(os.path.join(folder_path, '*'))

  # Lặp qua các file
  dfs = []
  start_date = datetime.strptime(start_date, '%Y-%m-%d').date()
  end_date = datetime.strptime(end_date, '%Y-%m-%d').date()
  today = date.today()
  path = '/content/drive/MyDrive/VN-QA/12.Fake Fail/FF_Pickup/FakeFail_PP/08.TID_data_weekly'
  files = glob.glob(os.path.join(path, '*'))
  for i in files:
    if i.split('.')[-1] == 'csv':
      tmp = i.split('/')[-1].split('.')[0]
      # tmp = pd.Timestamp(tmp.strftime("%Y-%m-%d"))
      tmp = datetime.strptime(tmp, '%Y-%m-%d').date()
      if (tmp >= start_date) & (tmp <= end_date):
        f = pd.read_csv(i)
        dfs.append(f)

  df1 = pd.concat(dfs)
  df1['attempt_date'] = pd.to_datetime(df1['attempt_date'])
  today = date.today()
  two_days_ago = today - timedelta(days=2)
  # month_today = date.today().strftime("%Y-%m")
  month_max = end_date.strftime('%Y-%m')
  if option == 'hub':
    m = pre_process(df1, lst_gr_hub)
    m = m.fillna(0)
    m.to_csv('/content/drive/MyDrive/VN-QA/12.Fake Fail/FF_Pickup/FakeFail_PP/09.TID_Final_data_monthly/final_hub/' + str(month_max) + '.csv', index=False)
  elif option == 'driver':
    m = pre_process(df1, lst_gr_driver)
    m = m.fillna(0)
    m.to_csv('/content/drive/MyDrive/VN-QA/12.Fake Fail/FF_Pickup/FakeFail_PP/09.TID_Final_data_monthly/final_driver/' + str(month_max) + '.csv', index=False)
  elif option == 'reason':
    total_reason_fail = pd.DataFrame(df1.groupby(lst_gr_driver)['reason'].value_counts()).rename(columns={'reason':'total_case'}).reset_index()
    total_reason_ff = pd.DataFrame(df1[df1['final_result']=='fake_fail'].groupby(lst_gr_driver)['reason'].value_counts()).rename(columns={'reason':'total_reason_ff'}).reset_index()
    reason = total_reason_fail.merge(total_reason_ff, how='outer', on=['attempt_date',  'region', 'hub_name', 'hub_id', 'driver_name', 'driver_id', 'reason'])
    reason = reason.fillna(0)
    reason.to_csv('/content/drive/MyDrive/VN-QA/12.Fake Fail/FF_Pickup/FakeFail_PP/09.TID_Final_data_monthly/final_reason/' + 'final_reason_data'+str(month_max) + '.csv', index=False)

def main(start_date, end_date):
  files = glob.glob(os.path.join('/content/drive/MyDrive/Fake Fail Result/Pickup Fake Fail Result/07-2023', '*'))
  for i in files:
    if (i.split('_')[-1]=='2.csv'):
      d = pd.read_csv(i)
      print(d.shape)
      read_last_7_days(d)

  save_final_data('hub', start_date, end_date)
  save_final_data('driver', start_date, end_date)
  save_final_data('reason',start_date, end_date)

main("2023-07-01", "2023-07-31")

# # alo1 = gc.open_by_url('https://docs.google.com/spreadsheets/d/1mxt0ngv-m9D9tqx1uzl08d0J46mLbDExDpAy7adRV1w/edit#gid=681689770').worksheet('QA_EXCLUDE_UPDATEON_12-06-2023')
# alo1 = get_as_dataframe(alo1, evaluate_formulas=True, header=0)

# alo2 = gc.open_by_url('https://docs.google.com/spreadsheets/d/1mxt0ngv-m9D9tqx1uzl08d0J46mLbDExDpAy7adRV1w/edit#gid=681689770').worksheet('QA_EXCLUDE_UPDATEON_24-05-2022')
# alo2 = get_as_dataframe(alo2, evaluate_formulas=True, header=0)

# beta1 = alo1['ID'].unique()
# beta2 = alo2['ID'].unique()

# df.loc[df['driver_id'].isin(beta1), 'fully_driver_result']='need_to_check'
# df.loc[df['driver_id'].isin(beta2), 'fully_driver_result']='need_to_check'

# dispute = gc.open_by_url('https://docs.google.com/spreadsheets/d/1i1Rha9Qg1qZ9sGI0-ddX9QBlO6Jg9URy2tm62Fu3X20/edit?resourcekey#gid=1021540264').worksheet('Detail_PickUp')
# dispute = get_as_dataframe(dispute, evaluate_formulas=True, header=0)

# dispute = dispute[dispute['Status']=='Corrected']

# rsvn = dispute['RSVN_id'].unique()

# df.loc[df['rsvn_id'].isin(rsvn), 'fully_driver_result']='need_to_check'

# fake_fail = df[df['fully_driver_result']=='fake_fail']

# tuSo = fake_fail.groupby(['driver_id', 'driver_name', 'hub_id',
#        'hub_name', 'region'])['order_id'].nunique().reset_index().rename(columns={'order_id':'total_order_FF'})

# mSo.rename(columns={'driver_hub_name':'hub_name', 'driver_hub_id':'hub_id', 'driver_hub_region':'region'}, inplace=True)

# mSo[mSo['driver_id']==1595441]

# hehe = mSo.merge(tuSo, how='left', on=['driver_id', 'driver_name', 'hub_id', 'hub_name', 'region'])

# hehe[hehe['driver_id']==1595441]

# hehe.fillna(0, inplace=True)

# hehe.to_csv('data.csv', index=False)

# hehe[hehe['total_order_FF'] >= hehe['total_orders']]

# hehe[hehe['driver_id']==1595441]



# data = tuSo.merge(mSo, on=['driver_id', 'driver_name', 'hub_id', 'hub_name', 'region'], how='outer')

# data = data.fillna(0)

# hi = pd.concat(dfs)

# shop_b2c = gc.open_by_url('https://docs.google.com/spreadsheets/d/165yaPv4ujgG-jr9klujlqtEn1Xi1Yk3M9eDq6IuosmM/edit#gid=0').worksheet('Sheet1')
# shop_b2c = get_as_dataframe(shop_b2c, evaluate_formulas=True, header=0)
# shop_b2c = shop_b2c[['shipper_id']]
# shop_b2c = shop_b2c[~shop_b2c['shipper_id'].isna()]
# lst_b2c = shop_b2c['shipper_id'].unique()

# hi=hi[~(hi['shipper_id'].isin(lst_b2c))]

# hi[(hi['driver_name']=='1239-NEXT-NH Hoang') & (hi['fully_driver_result']=='fake_fail') ]

# mSo[mSo['driver_name']=='1239-NEXT-NV Hoang']

# data[data['total_order_FF'] >= data['total_orders']]

# tuSo[tuSo['driver_name']=='1239-NEXT-NV Hoang']

